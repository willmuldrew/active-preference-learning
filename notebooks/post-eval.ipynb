{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "794d1ffd-bd2e-44fa-93ec-443e8e803c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54a9eacc-f211-4bd6-b7d3-03b4172f1edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import json\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "sys.path.append(\"../.\")\n",
    "import direct.openai_ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28ab7b0e-c774-4d79-88eb-2a39f7a786ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = \"imdb\"\n",
    "# project = \"wm-debug-imdb\"\n",
    "# tag = \"xmas-sweep2\"\n",
    "# m_s = [128, 256, 512, 768]\n",
    "# phase_s = [0, 1, 3, 5]\n",
    "\n",
    "dataset = \"tldr\"\n",
    "project = \"wm-debug-tldr\"\n",
    "tag = \"xmas-sweep2\"\n",
    "m_s = [128, 256, 384, 512]\n",
    "phase_s = [1, 2, 3, 4]\n",
    "\n",
    "\n",
    "n = 1024\n",
    "# n = 64\n",
    "\n",
    "# dataset = \"tldr\"\n",
    "# project = \"wm-debug-tldr\"\n",
    "# tag = \"lora-sweep2\"\n",
    "#m_s = [512]\n",
    "# m_s = [128,256, 384, 512, 640, 768]\n",
    "# m_s = [512]\n",
    "# m_s = [768]\n",
    "#n = 1024\n",
    "# n = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a47aee94-59c6-40f3-853b-27b75f6b71c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval_data(project, tag, filename):\n",
    "    dfs = []\n",
    "    for r in wandb.Api().runs(project, filters=dict(tags=tag)):\n",
    "        if tag in r.tags:\n",
    "            cfg = json.loads(r.json_config)\n",
    "            for f in r.files():\n",
    "                if f.name == filename:\n",
    "                    print(\"loading\", f.name, \"from\", r.name)\n",
    "                    root = \"/tmp\"\n",
    "                    f.download(root, replace=True)\n",
    "                    path = f\"{root}/{f.name}\"\n",
    "                    eval_data = json.loads(open(path).read())\n",
    "\n",
    "                    df = pd.DataFrame(eval_data)\n",
    "                    df[\"acquire_pairs_function\"] = cfg[\"exp5\"][\"value\"][\"acquire_pairs_function\"]\n",
    "                    #df[\"og\"] = int(cfg[\"exp5\"][\"value\"][\"over_generate_factor\"])\n",
    "                    df[\"seed\"] = int(cfg[\"seed\"][\"value\"])\n",
    "                    df[\"run_name\"] = r.name\n",
    "                    dfs.append(df)\n",
    "\n",
    "    return pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56824c16-ad95-4279-b33d-71967b52e51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_to_oracle(eval_filename):\n",
    "    df = get_eval_data(project, tag, eval_filename)\n",
    "    df = df[df.acquire_pairs_function == \"HIGH_ENTROPY\"]\n",
    "    #df = df[~df.seed.isin([12029, 22151, 22371, 23767, 29799])]\n",
    "    if m == 0:\n",
    "        # No point paying to eval other variants since these are sampled from the ref model!\n",
    "        df = df[df.acquire_pairs_function == \"RANDOM\"]\n",
    "\n",
    "    print(set(df.seed))\n",
    "    print(set(df.acquire_pairs_function))    \n",
    "    \n",
    "    sample_df = df.groupby([\"acquire_pairs_function\", \"seed\"], group_keys=False).apply(lambda x: x.sample(n))\n",
    "    batch = sample_df[[\"prompts\", \"completions\", \"vs_completions\"]].rename(\n",
    "    columns={\"completions\": \"completion_a\", \"vs_completions\": \"completion_b\", \"prompts\": \"prompt\"}).to_dict(\"records\")\n",
    "    \n",
    "    oracle_response = direct.openai_ranking.get_preference_batch(batch, \"gpt-4-1106-preview\", None, 10, dataset, provider=\"openai\")\n",
    "    cost = sum([r['cost'] for r in oracle_response])\n",
    "    print(f\"That cost ~ {cost} USD\")\n",
    "    sample_df[\"win\"] = [r['preferred'] == 0 for r in oracle_response]  \n",
    "    return sample_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "825022ca-4a7b-4948-b9ec-bcde132291f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, os\n",
    "\n",
    "def to_csv(df, filename):\n",
    "    if os.path.exists(filename):\n",
    "        print(f\"Warning - {filename} exists\")\n",
    "        filename = filename + \".\" + str(time.time())\n",
    "        print(f\"Writing to {filename}\")\n",
    "        \n",
    "    df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fa8c302-a310-4744-9e3e-ec0e7e43ead5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading evaluation_m128_phase1_post_training_T0.25.json from sparkling-haze-260\n",
      "loading evaluation_m128_phase1_post_training_T0.25.json from distinctive-star-259\n",
      "loading evaluation_m128_phase1_post_training_T0.25.json from faithful-breeze-258\n",
      "loading evaluation_m128_phase1_post_training_T0.25.json from classic-grass-257\n",
      "loading evaluation_m128_phase1_post_training_T0.25.json from balmy-dragon-256\n",
      "loading evaluation_m128_phase1_post_training_T0.25.json from floral-plasma-255\n",
      "{6242, 9242, 7242, 8242, 4242, 5242}\n",
      "{'HIGH_ENTROPY'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting preferences of batch of 6144 using 10 threads: 100%|██████████| 6144/6144 [28:55<00:00,  3.54it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That cost ~ 34.985630000000036 USD\n",
      "Warning - ../results/post-eval-winrate-tldr-m128-xmas-sweep2.csv exists\n",
      "Writing to ../results/post-eval-winrate-tldr-m128-xmas-sweep2.csv.1717145431.1338885\n",
      "loading evaluation_m256_phase2_post_training_T0.25.json from sparkling-haze-260\n",
      "loading evaluation_m256_phase2_post_training_T0.25.json from distinctive-star-259\n",
      "loading evaluation_m256_phase2_post_training_T0.25.json from faithful-breeze-258\n",
      "loading evaluation_m256_phase2_post_training_T0.25.json from classic-grass-257\n",
      "loading evaluation_m256_phase2_post_training_T0.25.json from balmy-dragon-256\n",
      "loading evaluation_m256_phase2_post_training_T0.25.json from floral-plasma-255\n",
      "{6242, 9242, 7242, 8242, 4242, 5242}\n",
      "{'HIGH_ENTROPY'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting preferences of batch of 6144 using 10 threads: 100%|██████████| 6144/6144 [30:47<00:00,  3.33it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That cost ~ 35.36103 USD\n",
      "Warning - ../results/post-eval-winrate-tldr-m256-xmas-sweep2.csv exists\n",
      "Writing to ../results/post-eval-winrate-tldr-m256-xmas-sweep2.csv.1717147291.832279\n",
      "loading evaluation_m384_phase3_post_training_T0.25.json from sparkling-haze-260\n",
      "loading evaluation_m384_phase3_post_training_T0.25.json from distinctive-star-259\n",
      "loading evaluation_m384_phase3_post_training_T0.25.json from faithful-breeze-258\n",
      "loading evaluation_m384_phase3_post_training_T0.25.json from classic-grass-257\n",
      "loading evaluation_m384_phase3_post_training_T0.25.json from balmy-dragon-256\n",
      "loading evaluation_m384_phase3_post_training_T0.25.json from floral-plasma-255\n",
      "{6242, 9242, 7242, 8242, 4242, 5242}\n",
      "{'HIGH_ENTROPY'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting preferences of batch of 6144 using 10 threads:  68%|██████▊   | 4149/6144 [20:46<08:58,  3.70it/s]  Traceback (most recent call last):\n",
      "  File \"/home/will/code/active-preference-learning/venv/lib/python3.10/site-packages/openai/api_requestor.py\", line 753, in _interpret_response_line\n",
      "    data = json.loads(rbody)\n",
      "  File \"/home/will/miniconda3/lib/python3.10/json/__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/home/will/miniconda3/lib/python3.10/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/home/will/miniconda3/lib/python3.10/json/decoder.py\", line 355, in raw_decode\n",
      "    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n",
      "json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/will/code/active-preference-learning/direct/openai_ranking.py\", line 172, in get_preference\n",
      "    resp = openai.ChatCompletion.create(\n",
      "  File \"/home/will/code/active-preference-learning/venv/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/home/will/code/active-preference-learning/venv/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/home/will/code/active-preference-learning/venv/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/home/will/code/active-preference-learning/venv/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/home/will/code/active-preference-learning/venv/lib/python3.10/site-packages/openai/api_requestor.py\", line 755, in _interpret_response_line\n",
      "    raise error.APIError(\n",
      "openai.error.APIError: HTTP code 502 from API (<html>\r\n",
      "<head><title>502 Bad Gateway</title></head>\r\n",
      "<body>\r\n",
      "<center><h1>502 Bad Gateway</h1></center>\r\n",
      "<hr><center>cloudflare</center>\r\n",
      "</body>\r\n",
      "</html>\r\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openai error raised HTTP code 502 from API (<html>\r\n",
      "<head><title>502 Bad Gateway</title></head>\r\n",
      "<body>\r\n",
      "<center><h1>502 Bad Gateway</h1></center>\r\n",
      "<hr><center>cloudflare</center>\r\n",
      "</body>\r\n",
      "</html>\r\n",
      ") - sleeping for 51.86881350489081 s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting preferences of batch of 6144 using 10 threads: 100%|██████████| 6144/6144 [30:41<00:00,  3.34it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That cost ~ 35.18677999999996 USD\n",
      "Warning - ../results/post-eval-winrate-tldr-m384-xmas-sweep2.csv exists\n",
      "Writing to ../results/post-eval-winrate-tldr-m384-xmas-sweep2.csv.1717149145.5653675\n",
      "loading evaluation_m512_phase4_post_training_T0.25.json from sparkling-haze-260\n",
      "loading evaluation_m512_phase4_post_training_T0.25.json from distinctive-star-259\n",
      "loading evaluation_m512_phase4_post_training_T0.25.json from faithful-breeze-258\n",
      "loading evaluation_m512_phase4_post_training_T0.25.json from classic-grass-257\n",
      "loading evaluation_m512_phase4_post_training_T0.25.json from balmy-dragon-256\n",
      "loading evaluation_m512_phase4_post_training_T0.25.json from floral-plasma-255\n",
      "{6242, 9242, 7242, 8242, 4242, 5242}\n",
      "{'HIGH_ENTROPY'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting preferences of batch of 6144 using 10 threads:  68%|██████▊   | 4148/6144 [20:27<09:04,  3.67it/s]  Traceback (most recent call last):\n",
      "  File \"/home/will/code/active-preference-learning/direct/openai_ranking.py\", line 172, in get_preference\n",
      "    resp = openai.ChatCompletion.create(\n",
      "  File \"/home/will/code/active-preference-learning/venv/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/home/will/code/active-preference-learning/venv/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/home/will/code/active-preference-learning/venv/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/home/will/code/active-preference-learning/venv/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/home/will/code/active-preference-learning/venv/lib/python3.10/site-packages/openai/api_requestor.py\", line 763, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.APIError: The server had an error while processing your request. Sorry about that! {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error while processing your request. Sorry about that!\",\n",
      "    \"type\": null,\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "} 500 {'error': {'message': 'The server had an error while processing your request. Sorry about that!', 'type': None, 'param': None, 'code': None}} {'Date': 'Fri, 31 May 2024 10:13:16 GMT', 'Content-Type': 'application/json', 'Content-Length': '165', 'Connection': 'keep-alive', 'openai-organization': 'ucl-ai-centre', 'openai-processing-ms': '30853', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '2000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '1999469', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '15ms', 'x-request-id': 'req_93662be9bf0a30e42d1987d75111ca1c', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '88c62ffa69286337-LHR', 'alt-svc': 'h3=\":443\"; ma=86400'}\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/will/code/active-preference-learning/direct/openai_ranking.py\", line 172, in get_preference\n",
      "    resp = openai.ChatCompletion.create(\n",
      "  File \"/home/will/code/active-preference-learning/venv/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/home/will/code/active-preference-learning/venv/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/home/will/code/active-preference-learning/venv/lib/python3.10/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/home/will/code/active-preference-learning/venv/lib/python3.10/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/home/will/code/active-preference-learning/venv/lib/python3.10/site-packages/openai/api_requestor.py\", line 763, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.APIError: The server had an error while processing your request. Sorry about that! {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error while processing your request. Sorry about that!\",\n",
      "    \"type\": null,\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "} 500 {'error': {'message': 'The server had an error while processing your request. Sorry about that!', 'type': None, 'param': None, 'code': None}} {'Date': 'Fri, 31 May 2024 10:13:17 GMT', 'Content-Type': 'application/json', 'Content-Length': '165', 'Connection': 'keep-alive', 'openai-organization': 'ucl-ai-centre', 'openai-processing-ms': '31219', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '2000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '1999517', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '14ms', 'x-request-id': 'req_ba5e3a63678a59af93f14e586739746d', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '88c62ff84f9edd80-LHR', 'alt-svc': 'h3=\":443\"; ma=86400'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openai error raised The server had an error while processing your request. Sorry about that! {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error while processing your request. Sorry about that!\",\n",
      "    \"type\": null,\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "} 500 {'error': {'message': 'The server had an error while processing your request. Sorry about that!', 'type': None, 'param': None, 'code': None}} {'Date': 'Fri, 31 May 2024 10:13:16 GMT', 'Content-Type': 'application/json', 'Content-Length': '165', 'Connection': 'keep-alive', 'openai-organization': 'ucl-ai-centre', 'openai-processing-ms': '30853', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '2000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '1999469', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '15ms', 'x-request-id': 'req_93662be9bf0a30e42d1987d75111ca1c', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '88c62ffa69286337-LHR', 'alt-svc': 'h3=\":443\"; ma=86400'} - sleeping for 51.76626759793599 s...\n",
      "openai error raised The server had an error while processing your request. Sorry about that! {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error while processing your request. Sorry about that!\",\n",
      "    \"type\": null,\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "} 500 {'error': {'message': 'The server had an error while processing your request. Sorry about that!', 'type': None, 'param': None, 'code': None}} {'Date': 'Fri, 31 May 2024 10:13:17 GMT', 'Content-Type': 'application/json', 'Content-Length': '165', 'Connection': 'keep-alive', 'openai-organization': 'ucl-ai-centre', 'openai-processing-ms': '31219', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '2000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '1999517', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '14ms', 'x-request-id': 'req_ba5e3a63678a59af93f14e586739746d', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '88c62ff84f9edd80-LHR', 'alt-svc': 'h3=\":443\"; ma=86400'} - sleeping for 45.289133578717774 s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting preferences of batch of 6144 using 10 threads: 100%|██████████| 6144/6144 [30:52<00:00,  3.32it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That cost ~ 35.28268000000006 USD\n",
      "Warning - ../results/post-eval-winrate-tldr-m512-xmas-sweep2.csv exists\n",
      "Writing to ../results/post-eval-winrate-tldr-m512-xmas-sweep2.csv.1717151010.3497448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for m, p in zip(m_s, phase_s):\n",
    "    eval_filename = f\"evaluation_m{m}_phase{p}_post_training_T0.25.json\"\n",
    "    s = submit_to_oracle(eval_filename)\n",
    "    to_csv(s, f\"../results/post-eval-winrate-{dataset}-m{m}-{tag}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c027c67-1857-42a1-bb55-3d8890569bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for m in [128, 256, 512, 768]:\n",
    "#     s = submit_to_oracle(m)\n",
    "#     s.to_csv(f\"../results/post-eval-winrate-{dataset}-m{m}-{tag}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30729fe-77ff-4efe-9cdb-d0976e293b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - how can I only eval the seeds I haven't seen before?  e.g. for high ent tldr?\n",
    "\n",
    "# for m in m_s:\n",
    "#     s = submit_to_oracle(m)\n",
    "#     s.to_csv(f\"../results/post-eval-winrate-{dataset}-m{m}-{tag}-ENTROPY.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a955ae-51ec-42fc-9f5c-e0d00563a85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../results/*xmas-lora.csv*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de506ff7-f9e1-4dd4-9edb-957eb4816a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = \"../results/*xmas-lora.csv*\"\n",
    "\n",
    "dfs = []\n",
    "for p in glob.glob(pattern):\n",
    "    m = int(p.split(\"-\")[4][1:])\n",
    "    df = pd.read_csv(p)\n",
    "    df[\"m\"] = m\n",
    "    dfs.append(df)\n",
    "df = pd.concat(dfs)\n",
    "df\n",
    "df.groupby([\"m\", \"acquire_pairs_function\"]).win.agg([\"count\", \"mean\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc2e906-0204-4eb6-9dea-2a1578ed5487",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
